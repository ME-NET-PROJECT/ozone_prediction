{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66d9bff0-389c-413b-ab34-2b11244c84ec",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c2ca982-05ba-4372-b284-80849addcbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, calendar \n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, LSTM, Dense,\n",
    "    Bidirectional, GRU, Add\n",
    ")\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import callbacks\n",
    "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, r2_score,\n",
    "    accuracy_score, precision_score, recall_score, f1_score\n",
    ")\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b74ec48-5216-4cfe-a9a0-34849fa92c47",
   "metadata": {},
   "source": [
    "## 2. Parameter Initialisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f4b90df-3c00-43c4-9154-8e689d39113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = f\"datasets/UK/\"\n",
    "dataset = \"sentinel_cams_aurn.csv\"\n",
    "model_dataset = \"aurn_cam\"\n",
    "models_folder = Path(dataset_folder) / Path(model_dataset) /Path(\"models\")\n",
    "stations_file = Path(dataset_folder) / \"monitoring_stations.csv\"\n",
    "model_weight_file = \"ozone_model_lstm.weights.h5\"\n",
    "sequence_length=5\n",
    "\n",
    "req_cols = [\n",
    "    'Time', 'SiteNumber', 'Longitude', 'Latitude', \n",
    "    'aurn_go3_max', 'aurn_no2', 'ch4_c', 'uvbed', 'uvbedcs', 't', 'ws' # remove ch4_c \n",
    "]\n",
    "\n",
    "engineer_cols=['aurn_go3_max', 'aurn_no2', 'ch4_c', 'uvbed', 'uvbedcs', 't', 'ws']\n",
    "\n",
    "exclude_cols = ['Time', 'SiteNumber', 'Longitude', 'Latitude']\n",
    "\n",
    "y_columns=['aurn_go3_max']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3951fdd8-4d1e-49f6-a812-6c0cc08fc7d6",
   "metadata": {},
   "source": [
    "## 3. A Function to Get the AURN site Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e14c139e-7c32-46d9-afbc-5ed2a557d397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_site_info(site_number, stations_file, site_column=\"SiteNumber\"):\n",
    "    try:\n",
    "        # Load the CSV\n",
    "        stations_df = pd.read_csv(stations_file)\n",
    "        \n",
    "        # Ensure SiteNumber is treated as a string\n",
    "        stations_df[site_column] = stations_df[site_column].astype(str)\n",
    "        site_number = str(site_number)\n",
    "\n",
    "        # Find the matching row\n",
    "        site_info = stations_df[stations_df[site_column] == site_number]\n",
    "        \n",
    "        if not site_info.empty:\n",
    "            site_info = site_info.iloc[0]  # Get the first match\n",
    "            return site_info[\"SiteName\"], site_info[\"Longitude\"], site_info[\"Latitude\"]\n",
    "        else:\n",
    "            return None, None, None  # Return None if not found\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {stations_file} not found.\")\n",
    "        return None, None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading site info: {e}\")\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd4fd36-38d6-43e5-aeed-c7c72963d1cc",
   "metadata": {},
   "source": [
    "## 4. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e80c8302-95b7-4910-a51b-fbc5f419bec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(Path(dataset_folder)/Path(dataset), parse_dates=['Time'], dayfirst=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d638731c-86e5-489d-a1a6-adbbf53abf5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 238.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['aurn_go3_max'].min(), df['aurn_go3_max'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b659d38-4a37-47bc-b59c-ec0f70942b78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.0, 157.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['aurn_go3'].min(), df['aurn_go3'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d916314-96ad-443c-aecd-7b7f639d3026",
   "metadata": {},
   "source": [
    "#### Compute Wind Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e973460-4609-4ac8-adc5-5539fe959c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ws'] = np.sqrt(df['u']**2 + df['v']**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25791344-41e6-484f-8a06-7e9566b974ad",
   "metadata": {},
   "source": [
    "#### Filter Required Site [OPTIONAL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95eb1d2e-3713-4ea6-a948-b81a02edf44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to train a subset of the data by site\n",
    "# df = df[df['SiteName'] == 'Barnsley Gawber']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079a4249-bf73-4800-a18d-616aea5dc928",
   "metadata": {},
   "source": [
    "#### Include Only Required Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3b1e313-ec58-4362-a158-6816bdbe7b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[req_cols].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21a3162c-1ebf-4b2c-a5b7-4426c21f5691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['aurn_go3_max'] = np.log1p(df['aurn_go3_max'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578ed661-1441-4bba-b3fa-f97af5030a07",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89425e9-a3d5-4ea1-abc0-a4083e309c65",
   "metadata": {},
   "source": [
    "### Polynomial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fedb60b-c7a4-49d5-ba5d-5d105c186f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def generate_polynomial_features(\n",
    "    df, degree=2, interaction_only=False, include_bias=False, include_columns=None\n",
    "):\n",
    "    # Handle columns to include\n",
    "    include_columns = include_columns or df.columns.tolist()  # Default to all columns if None\n",
    "    included_df = df[include_columns]  # DataFrame with only included columns\n",
    "    excluded_df = df.drop(columns=include_columns, errors='ignore')  # Columns not used for polynomial features\n",
    "\n",
    "    # Generate polynomial features on the included columns\n",
    "    poly = PolynomialFeatures(degree=degree, interaction_only=interaction_only, include_bias=include_bias)\n",
    "    poly_features = poly.fit_transform(included_df.values)\n",
    "    feature_names = poly.get_feature_names_out(included_df.columns)\n",
    "    \n",
    "    # Create DataFrame for polynomial features\n",
    "    poly_df = pd.DataFrame(poly_features, columns=feature_names, index=df.index)\n",
    "    \n",
    "    # Concatenate excluded columns back with the polynomial features\n",
    "    final_df = pd.concat([poly_df, excluded_df], axis=1)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "\n",
    "df = generate_polynomial_features(\n",
    "    df, \n",
    "    degree=2, \n",
    "    interaction_only=False, \n",
    "    include_bias=False, \n",
    "    include_columns=engineer_cols\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87667ef1-95e5-4a2e-858c-8a31fd0fbd98",
   "metadata": {},
   "source": [
    "### Temporal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ff35d4d-b084-4568-a21a-1424feded529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_days_in_year(year):\n",
    "    return 366 if calendar.isleap(year) else 365\n",
    "        \n",
    "def temporal_cyclical_features(df, time_col='Time'):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Convert the time column to datetime if not already\n",
    "    if not np.issubdtype(df[time_col].dtype, np.datetime64):\n",
    "        df[time_col] = pd.to_datetime(df[time_col])\n",
    "\n",
    "    # Extract temporal features\n",
    "    df['Year'] = df[time_col].dt.year\n",
    "    df['DayOfWeek'] = df[time_col].dt.weekday                   # Day of the week (0-6)\n",
    "    df['DayOfYear'] = df[time_col].dt.dayofyear                 # Day of the year (1-365/366)\n",
    "    df['Month'] = df[time_col].dt.month                         # Month of the year (1-12)\n",
    "    df['IsWeekend'] = df[time_col].dt.weekday.apply(lambda x: 1 if x >= 5 else 0)  # Weekday vs Weekend\n",
    "    # Extract ISO Week Number (WeekOfYear)\n",
    "    df['WeekOfYear'] = df[time_col].dt.isocalendar().week\n",
    "    # Handle Week 53 cases by mapping them to 52\n",
    "    df['WeekOfYear'] = df['WeekOfYear'].apply(lambda x: x if x <= 52 else 52)\n",
    "\n",
    "    # Compute the Lunar Phase (0 = New Moon, 14-15 = Full Moon, 29 = Next New Moon)\n",
    "    df['LunarDay'] = (df[time_col] - pd.Timestamp(\"2000-01-06\")).dt.days % 29.53  # Reference New Moon date\n",
    "\n",
    "    # Determine the season (0: Winter, 1: Spring, 2: Summer, 3: Fall)\n",
    "    df['Season'] = df[time_col].apply(lambda x: \n",
    "        0 if x.month in [12, 1, 2] else \n",
    "        1 if x.month in [3, 4, 5] else \n",
    "        2 if x.month in [6, 7, 8] else \n",
    "        3\n",
    "    )\n",
    "\n",
    "    df['DaysInYear'] = df['Year'].apply(get_days_in_year)\n",
    "\n",
    "    # Cyclical encoding for the day of the year\n",
    "    df['DayOfYearSin'] = np.sin(2 * np.pi * df['DayOfYear'] / df['DaysInYear'])\n",
    "    df['DayOfYearCos'] = np.cos(2 * np.pi * df['DayOfYear'] / df['DaysInYear'])\n",
    "\n",
    "    df['DayOfWeekSin'] = np.sin(2 * np.pi * df['DayOfWeek'] / 7)\n",
    "    df['DayOfWeekCos'] = np.cos(2 * np.pi * df['DayOfWeek'] / 7)\n",
    "\n",
    "    df['MonthSin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
    "    df['MonthCos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
    "\n",
    "    df['WeekOfYearSin'] = np.sin(2 * np.pi * df['WeekOfYear'] / 52)\n",
    "    df['WeekOfYearCos'] = np.cos(2 * np.pi * df['WeekOfYear'] / 52)\n",
    "\n",
    "    # Cyclical Encoding of the Lunar Cycle\n",
    "    df['LunarSin'] = np.sin(2 * np.pi * df['LunarDay'] / 29.53)\n",
    "    df['LunarCos'] = np.cos(2 * np.pi * df['LunarDay'] / 29.53)\n",
    "\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "df = temporal_cyclical_features(df, time_col='Time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2dd192-a226-432d-bb80-f549e9aee191",
   "metadata": {},
   "source": [
    "### Min/Max Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4b72e4c-abd9-44de-91c7-c0e3bb9446d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scale_features(df, y_columns, exclude_x_columns=None):\n",
    "    # Separate features and target\n",
    "    if exclude_x_columns:\n",
    "        y_columns = y_columns + exclude_x_columns\n",
    "        \n",
    "    features = df.drop(columns=y_columns)\n",
    "    target = df[y_columns]\n",
    "\n",
    "    # Apply Min-Max Scaling to features\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_features = scaler.fit_transform(features)\n",
    "    \n",
    "    # Convert back to DataFrame and retain original column names\n",
    "    scaled_features_df = pd.DataFrame(scaled_features, columns=features.columns, index=df.index)\n",
    "    \n",
    "    # Combine scaled features with the target column\n",
    "    result_df = pd.concat([scaled_features_df, target], axis=1)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "\n",
    "df = min_max_scale_features(df, y_columns=y_columns, exclude_x_columns=exclude_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134a1644-ce15-41f5-a9f6-1ca9a98f9255",
   "metadata": {},
   "source": [
    "### Split the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8c00bcd-0540-49fe-9dde-909ab3d1e817",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start = '2019-01-01'\n",
    "train_end = '2023-12-31'\n",
    "\n",
    "# The training set split (includes train and validation)\n",
    "df_train = df[(df['Time'] >= train_start) & (df['Time'] <= train_end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "476b03ec-e568-48b7-90de-9b89dc713b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_start = '2024-01-01'\n",
    "test_end = '2024-08-31'\n",
    "\n",
    "# The training set split (includes train and validation)\n",
    "df_test = df[(df['Time'] >= test_start) & (df['Time'] <= test_end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7a6ae7-b9ad-4e5f-b4ad-56673752e673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48419f20-52c0-43f7-885a-672b75106be7",
   "metadata": {},
   "source": [
    "### Create Training Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4213fa27-f49a-4607-8ea3-8db479720774",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sites: 100%|████████████████████████████████████████████████████████████████| 80/80 [02:31<00:00,  1.89s/it]\n",
      "Processing Sites: 100%|████████████████████████████████████████████████████████████████| 78/78 [00:17<00:00,  4.39it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "def create_sequences(df, seq_length, y_columns):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    metadata = []  # List to store Time and SiteNumber information\n",
    "    \n",
    "    for i in range(len(df) - seq_length):\n",
    "        seq = df.iloc[i:i + seq_length]\n",
    "        # Check if the sequence has consecutive days\n",
    "        time_diff = (seq['Time'].diff().dropna() == pd.Timedelta(days=1)).all()\n",
    "        \n",
    "        if not time_diff:  # Skip sequences with non-consecutive days\n",
    "            continue\n",
    "\n",
    "        # Include lagged y_column(s) as features within the sequence\n",
    "        lagged_features = seq[y_columns].values  # Extract the lagged `y_columns`\n",
    "        other_features = seq[df.columns.drop(['Time', 'SiteNumber', *y_columns])].values\n",
    "        sequence_features = np.hstack((other_features, lagged_features))  # Concatenate features\n",
    "\n",
    "        \n",
    "        # Append the valid sequence and target\n",
    "        sequences.append(seq[df.columns.drop(['Time', 'SiteNumber'])].values)\n",
    "        targets.append(df.iloc[i + seq_length][y_columns])\n",
    "        \n",
    "        # Store metadata for sequence\n",
    "        metadata.append({\n",
    "            \"Start_Time\": seq.iloc[0]['Time'],  # Start time of the sequence\n",
    "            \"End_Time\": seq.iloc[-1]['Time'],  # End time of the sequence\n",
    "            \"SiteNumber\": seq.iloc[0]['SiteNumber'],  # Site number for the sequence\n",
    "        })\n",
    "    \n",
    "    metadata_df = pd.DataFrame(metadata)\n",
    "    \n",
    "    return np.array(sequences), np.array(targets), metadata_df\n",
    "\n",
    "\n",
    "\n",
    "def generate_sequences_by_site(df, site_column, seq_length, y_columns):\n",
    "    all_sequences = []\n",
    "    all_targets = []\n",
    "    all_metadatas = []\n",
    "\n",
    "    # Iterate over each unique site\n",
    "    for site, group in tqdm(df.groupby(site_column), desc=\"Processing Sites\"):\n",
    "        # Sort the group by time\n",
    "        group = group.sort_values(by='Time')\n",
    "\n",
    "        # Create sequences for this group\n",
    "        site_sequences, site_targets, site_metadata = create_sequences(group, seq_length, y_columns)\n",
    "\n",
    "        # Append sequences and targets\n",
    "        if site_sequences.size > 0:  # Only add non-empty sequences\n",
    "            all_sequences.append(site_sequences)\n",
    "            all_targets.append(site_targets)\n",
    "            all_metadatas.append(site_metadata)\n",
    "\n",
    "    # Concatenate sequences and targets across all sites\n",
    "    if all_sequences:\n",
    "        all_sequences = np.concatenate(all_sequences, axis=0)\n",
    "        all_targets = np.concatenate(all_targets, axis=0)\n",
    "        all_metadatas = np.concatenate(all_metadatas, axis=0)\n",
    "    else:\n",
    "        all_sequences = np.array([])\n",
    "        all_targets = np.array([])\n",
    "        all_metadatas = np.array([])\n",
    "\n",
    "    # Return sequences and targets\n",
    "    return all_sequences, all_targets, all_metadatas\n",
    "\n",
    "\n",
    "X_train, y_train, X_train_meta = generate_sequences_by_site(df_train, 'SiteNumber', sequence_length, y_columns)\n",
    "X_test, y_test, X_test_meta = generate_sequences_by_site(df_test, 'SiteNumber', sequence_length, y_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c448638-eeda-41b7-b0af-d33df032e154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure data types are float32\n",
    "X_train = X_train.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "y_test = y_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1951a9d7-7c92-4400-bb43-11427ae02ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100244, 5, 56),\n",
       " (100244, 1),\n",
       " (100244, 3),\n",
       " (12352, 5, 56),\n",
       " (12352, 1),\n",
       " (12352, 3))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_train_meta.shape,  X_test.shape, y_test.shape, X_test_meta.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa675aa-65fc-4c97-be85-9d1352797467",
   "metadata": {},
   "source": [
    "## 6. ML Models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebca9a4c-9e19-4e52-9691-38d220390386",
   "metadata": {},
   "source": [
    "### 1. LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "45008f2c-bff8-4a1c-811e-b59a2720fb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM-based model\n",
    "def lstm_model(in_time_steps, in_features):\n",
    "    input_layer = Input(shape=(in_time_steps, in_features))\n",
    "\n",
    "    # First bi-directional LSTM layer\n",
    "    x =  LSTM(units=50, return_sequences=True)(input_layer)\n",
    "    x =  LSTM(units=50, return_sequences=False)(x)\n",
    "    \n",
    "    x = Dense(units=10, activation='relu')(x)\n",
    "    output_layer = Dense(1, activation='linear')(x)\n",
    "    \n",
    "    # Create the model\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb19c2cb-3d7e-4c7c-9bc4-019a9b6967a9",
   "metadata": {},
   "source": [
    "### 2. Bi-Directional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d69a0497-f083-4b6b-906f-05390d719cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bi-directional LSTM-based model\n",
    "def bi_lstm_model(in_time_steps, in_features):\n",
    "    input_layer = Input(shape=(in_time_steps, in_features))\n",
    "\n",
    "    # First bi-directional LSTM layer\n",
    "    x =  Bidirectional(LSTM(units=256, return_sequences=True))(input_layer)\n",
    "    \n",
    "    # Feed-forward layer on the sequence\n",
    "    x_ffn = Dense(units=512, activation='relu')(x)  # Apply Dense layer to sequence\n",
    "    x =  Bidirectional(LSTM(units=256, return_sequences=True))(x)\n",
    "    \n",
    "    # Residual connection: add bi-directional LSTM output and Dense output\n",
    "    x = Add()([x, x_ffn])\n",
    "    \n",
    "    # Final bi-directional LSTM layers for sequence compression\n",
    "    x =  Bidirectional(LSTM(units=128, return_sequences=True))(x)\n",
    "    x =  Bidirectional(LSTM(units=64, return_sequences=True))(x)\n",
    "    x =  Bidirectional(LSTM(units=32, return_sequences=False))(x)\n",
    "    \n",
    "    x = Dense(units=10, activation='relu')(x)\n",
    "    \n",
    "    # Output layer (regression target)\n",
    "    output_layer = Dense(1, activation='linear')(x)\n",
    "    \n",
    "    # Create the model\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8a598f-17fc-4986-8fe8-d666aa6849eb",
   "metadata": {},
   "source": [
    "### 3. Gated Recurrent Unit (GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c68c1621-f02e-457b-9021-e3117413c6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gated Recurrent Unit (GRU)\n",
    "def gru_model(in_time_steps, in_features):\n",
    "    input_layer = Input(shape=(in_time_steps, in_features))\n",
    "\n",
    "    # First bi-directional LSTM layer\n",
    "    x =  GRU(units=50, return_sequences=True)(input_layer)\n",
    "    x =  GRU(units=50, return_sequences=False)(x)\n",
    "    \n",
    "    x = Dense(units=10, activation='relu')(x)\n",
    "    \n",
    "    # Output layer (regression target)\n",
    "    output_layer = Dense(1, activation='linear')(x)\n",
    "    \n",
    "    # Create the model\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d29efe-9566-4808-816d-f9a604ca3c0a",
   "metadata": {},
   "source": [
    "### 4. Bi-Directional Gated Recurrent Unit (GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "62de2a39-fa8a-430d-bbc7-7db3d86ff447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bi-directional GRU-based model\n",
    "def bi_gru_model(in_time_steps, in_features):\n",
    "    input_layer = Input(shape=(in_time_steps, in_features))\n",
    "\n",
    "    # First bi-directional LSTM layer\n",
    "    x =  Bidirectional(GRU(units=256, return_sequences=True))(input_layer)\n",
    "    \n",
    "    # Feed-forward layer on the sequence\n",
    "    x_ffn = Dense(units=512, activation='relu')(x)  # Apply Dense layer to sequence\n",
    "    x =  Bidirectional(GRU(units=256, return_sequences=True))(x)\n",
    "    \n",
    "    # Residual connection: add bi-directional LSTM output and Dense output\n",
    "    x = Add()([x, x_ffn])\n",
    "    \n",
    "    # Final bi-directional LSTM layers for sequence compression\n",
    "    x =  Bidirectional(GRU(units=128, return_sequences=True))(x)\n",
    "    x =  Bidirectional(GRU(units=64, return_sequences=True))(x)\n",
    "    x =  Bidirectional(GRU(units=32, return_sequences=False))(x)\n",
    "    \n",
    "    x = Dense(units=10, activation='relu')(x)\n",
    "    \n",
    "    # Output layer (regression target)\n",
    "    output_layer = Dense(1, activation='linear')(x)\n",
    "    \n",
    "    # Create the model\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c5a260-f661-4f8f-aa35-92cf28804c53",
   "metadata": {},
   "source": [
    "### 4. Train and Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26157678-1fe5-49d3-97fd-6ea4f7e6a3e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training lstm model...\n",
      "Epoch 1/200\n",
      "3129/3133 [============================>.] - ETA: 0s - loss: 1.3029     \n",
      "Epoch 1: val_loss improved from inf to 0.92418, saving model to datasets\\UK\\aurn_cam\\models\\lstm.h5\n",
      "3133/3133 [==============================] - 34s 9ms/step - loss: 1.3025 - val_loss: 0.9242 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "3127/3133 [============================>.] - ETA: 0s - loss: 0.9523 \n",
      "Epoch 2: val_loss improved from 0.92418 to 0.90189, saving model to datasets\\UK\\aurn_cam\\models\\lstm.h5\n",
      "3133/3133 [==============================] - 27s 9ms/step - loss: 0.9522 - val_loss: 0.9019 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "3130/3133 [============================>.] - ETA: 0s - loss: 0.9366 \n",
      "Epoch 3: val_loss improved from 0.90189 to 0.86900, saving model to datasets\\UK\\aurn_cam\\models\\lstm.h5\n",
      "3133/3133 [==============================] - 26s 8ms/step - loss: 0.9366 - val_loss: 0.8690 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "3130/3133 [============================>.] - ETA: 0s - loss: 0.9288 \n",
      "Epoch 4: val_loss did not improve from 0.86900\n",
      "3133/3133 [==============================] - 29s 9ms/step - loss: 0.9287 - val_loss: 0.8780 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "3127/3133 [============================>.] - ETA: 0s - loss: 0.9246 \n",
      "Epoch 5: val_loss improved from 0.86900 to 0.85958, saving model to datasets\\UK\\aurn_cam\\models\\lstm.h5\n",
      "3133/3133 [==============================] - 26s 8ms/step - loss: 0.9245 - val_loss: 0.8596 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "3126/3133 [============================>.] - ETA: 0s - loss: 0.9202 \n",
      "Epoch 6: val_loss did not improve from 0.85958\n",
      "3133/3133 [==============================] - 26s 8ms/step - loss: 0.9203 - val_loss: 0.8876 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "3133/3133 [==============================] - ETA: 0s - loss: 0.9159 \n",
      "Epoch 7: val_loss did not improve from 0.85958\n",
      "3133/3133 [==============================] - 27s 9ms/step - loss: 0.9159 - val_loss: 0.8635 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "3129/3133 [============================>.] - ETA: 0s - loss: 0.9109 \n",
      "Epoch 8: val_loss did not improve from 0.85958\n",
      "3133/3133 [==============================] - 28s 9ms/step - loss: 0.9108 - val_loss: 0.8688 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "3129/3133 [============================>.] - ETA: 0s - loss: 0.9062 \n",
      "Epoch 9: val_loss did not improve from 0.85958\n",
      "3133/3133 [==============================] - 25s 8ms/step - loss: 0.9063 - val_loss: 0.8827 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "3131/3133 [============================>.] - ETA: 0s - loss: 0.9043 \n",
      "Epoch 10: val_loss did not improve from 0.85958\n",
      "3133/3133 [==============================] - 25s 8ms/step - loss: 0.9043 - val_loss: 0.8830 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "3132/3133 [============================>.] - ETA: 0s - loss: 0.8993 \n",
      "Epoch 11: val_loss did not improve from 0.85958\n",
      "3133/3133 [==============================] - 26s 8ms/step - loss: 0.8993 - val_loss: 0.8600 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "3128/3133 [============================>.] - ETA: 0s - loss: 0.8970 \n",
      "Epoch 12: val_loss did not improve from 0.85958\n",
      "3133/3133 [==============================] - 24s 8ms/step - loss: 0.8972 - val_loss: 0.8984 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "3128/3133 [============================>.] - ETA: 0s - loss: 0.8911 \n",
      "Epoch 13: val_loss did not improve from 0.85958\n",
      "3133/3133 [==============================] - 29s 9ms/step - loss: 0.8911 - val_loss: 0.9180 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "3131/3133 [============================>.] - ETA: 0s - loss: 0.8877 \n",
      "Epoch 14: val_loss did not improve from 0.85958\n",
      "3133/3133 [==============================] - 27s 8ms/step - loss: 0.8877 - val_loss: 0.8630 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "3133/3133 [==============================] - ETA: 0s - loss: 0.8816 \n",
      "Epoch 15: val_loss did not improve from 0.85958\n",
      "3133/3133 [==============================] - 28s 9ms/step - loss: 0.8816 - val_loss: 0.8659 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "3133/3133 [==============================] - ETA: 0s - loss: 0.8603 \n",
      "Epoch 16: val_loss did not improve from 0.85958\n",
      "3133/3133 [==============================] - 27s 9ms/step - loss: 0.8603 - val_loss: 0.8750 - lr: 1.0000e-04\n",
      "Epoch 17/200\n",
      "3133/3133 [==============================] - ETA: 0s - loss: 0.8570 \n",
      "Epoch 17: val_loss did not improve from 0.85958\n",
      "3133/3133 [==============================] - 27s 9ms/step - loss: 0.8570 - val_loss: 0.8731 - lr: 1.0000e-04\n",
      "Epoch 18/200\n",
      "3128/3133 [============================>.] - ETA: 0s - loss: 0.8558 \n",
      "Epoch 18: val_loss did not improve from 0.85958\n",
      "3133/3133 [==============================] - 27s 9ms/step - loss: 0.8557 - val_loss: 0.8759 - lr: 1.0000e-04\n",
      "Epoch 19/200\n",
      "3129/3133 [============================>.] - ETA: 0s - loss: 0.8542 \n",
      "Epoch 19: val_loss did not improve from 0.85958\n",
      "3133/3133 [==============================] - 28s 9ms/step - loss: 0.8543 - val_loss: 0.8784 - lr: 1.0000e-04\n",
      "Epoch 20/200\n",
      "3132/3133 [============================>.] - ETA: 0s - loss: 0.8530 \n",
      "Epoch 20: val_loss did not improve from 0.85958\n",
      "3133/3133 [==============================] - 28s 9ms/step - loss: 0.8530 - val_loss: 0.8784 - lr: 1.0000e-04\n",
      "lstm model saved!\n",
      "Training bi_lstm model...\n",
      "Epoch 1/200\n",
      "3132/3133 [============================>.] - ETA: 0s - loss: 1.2823     \n",
      "Epoch 1: val_loss improved from inf to 0.90988, saving model to datasets\\UK\\aurn_cam\\models\\bi_lstm.h5\n",
      "3133/3133 [==============================] - 80s 22ms/step - loss: 1.2822 - val_loss: 0.9099 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "3131/3133 [============================>.] - ETA: 0s - loss: 0.9739  \n",
      "Epoch 2: val_loss improved from 0.90988 to 0.87570, saving model to datasets\\UK\\aurn_cam\\models\\bi_lstm.h5\n",
      "3133/3133 [==============================] - 67s 21ms/step - loss: 0.9739 - val_loss: 0.8757 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "3133/3133 [==============================] - ETA: 0s - loss: 0.9571  \n",
      "Epoch 3: val_loss improved from 0.87570 to 0.86646, saving model to datasets\\UK\\aurn_cam\\models\\bi_lstm.h5\n",
      "3133/3133 [==============================] - 65s 21ms/step - loss: 0.9571 - val_loss: 0.8665 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "3131/3133 [============================>.] - ETA: 0s - loss: 0.9467  \n",
      "Epoch 4: val_loss did not improve from 0.86646\n",
      "3133/3133 [==============================] - 61s 19ms/step - loss: 0.9467 - val_loss: 0.8817 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "3132/3133 [============================>.] - ETA: 0s - loss: 0.9404  \n",
      "Epoch 5: val_loss did not improve from 0.86646\n",
      "3133/3133 [==============================] - 62s 20ms/step - loss: 0.9403 - val_loss: 0.8764 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "3133/3133 [==============================] - ETA: 0s - loss: 0.9338  \n",
      "Epoch 6: val_loss did not improve from 0.86646\n",
      "3133/3133 [==============================] - 65s 21ms/step - loss: 0.9338 - val_loss: 0.8905 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "3132/3133 [============================>.] - ETA: 0s - loss: 0.9281  \n",
      "Epoch 7: val_loss did not improve from 0.86646\n",
      "3133/3133 [==============================] - 63s 20ms/step - loss: 0.9281 - val_loss: 0.8788 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "3130/3133 [============================>.] - ETA: 0s - loss: 0.9224  \n",
      "Epoch 8: val_loss did not improve from 0.86646\n",
      "3133/3133 [==============================] - 61s 19ms/step - loss: 0.9223 - val_loss: 0.8724 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "3133/3133 [==============================] - ETA: 0s - loss: 0.9204  \n",
      "Epoch 9: val_loss improved from 0.86646 to 0.85326, saving model to datasets\\UK\\aurn_cam\\models\\bi_lstm.h5\n",
      "3133/3133 [==============================] - 61s 19ms/step - loss: 0.9204 - val_loss: 0.8533 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "3133/3133 [==============================] - ETA: 0s - loss: 0.9166  \n",
      "Epoch 10: val_loss did not improve from 0.85326\n",
      "3133/3133 [==============================] - 61s 19ms/step - loss: 0.9166 - val_loss: 0.9289 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "3130/3133 [============================>.] - ETA: 0s - loss: 0.9123  \n",
      "Epoch 11: val_loss did not improve from 0.85326\n",
      "3133/3133 [==============================] - 61s 19ms/step - loss: 0.9124 - val_loss: 0.8963 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "3132/3133 [============================>.] - ETA: 0s - loss: 0.9068  \n",
      "Epoch 12: val_loss did not improve from 0.85326\n",
      "3133/3133 [==============================] - 61s 19ms/step - loss: 0.9068 - val_loss: 0.8756 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "3131/3133 [============================>.] - ETA: 0s - loss: 0.9027  \n",
      "Epoch 13: val_loss did not improve from 0.85326\n",
      "3133/3133 [==============================] - 61s 19ms/step - loss: 0.9027 - val_loss: 0.8580 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "3131/3133 [============================>.] - ETA: 0s - loss: 0.8986  \n",
      "Epoch 14: val_loss did not improve from 0.85326\n",
      "3133/3133 [==============================] - 61s 19ms/step - loss: 0.8987 - val_loss: 0.9301 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "3132/3133 [============================>.] - ETA: 0s - loss: 0.8925  \n",
      "Epoch 15: val_loss did not improve from 0.85326\n",
      "3133/3133 [==============================] - 61s 19ms/step - loss: 0.8926 - val_loss: 0.8737 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "3131/3133 [============================>.] - ETA: 0s - loss: 0.8863  \n",
      "Epoch 16: val_loss did not improve from 0.85326\n",
      "3133/3133 [==============================] - 61s 19ms/step - loss: 0.8863 - val_loss: 0.8850 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "3133/3133 [==============================] - ETA: 0s - loss: 0.8836  \n",
      "Epoch 17: val_loss did not improve from 0.85326\n",
      "3133/3133 [==============================] - 61s 19ms/step - loss: 0.8836 - val_loss: 0.8755 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "3133/3133 [==============================] - ETA: 0s - loss: 0.8786  \n",
      "Epoch 18: val_loss did not improve from 0.85326\n",
      "3133/3133 [==============================] - 61s 19ms/step - loss: 0.8786 - val_loss: 0.8738 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "3133/3133 [==============================] - ETA: 0s - loss: 0.8709  \n",
      "Epoch 19: val_loss did not improve from 0.85326\n",
      "3133/3133 [==============================] - 61s 19ms/step - loss: 0.8709 - val_loss: 0.9102 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "3133/3133 [==============================] - ETA: 0s - loss: 0.8381  \n",
      "Epoch 20: val_loss did not improve from 0.85326\n",
      "3133/3133 [==============================] - 64s 20ms/step - loss: 0.8381 - val_loss: 0.8733 - lr: 1.0000e-04\n",
      "Epoch 21/200\n",
      "3130/3133 [============================>.] - ETA: 0s - loss: 0.8304  \n",
      "Epoch 21: val_loss did not improve from 0.85326\n",
      "3133/3133 [==============================] - 65s 21ms/step - loss: 0.8304 - val_loss: 0.8751 - lr: 1.0000e-04\n",
      "Epoch 22/200\n",
      "3133/3133 [==============================] - ETA: 0s - loss: 0.8262  \n",
      "Epoch 22: val_loss did not improve from 0.85326\n",
      "3133/3133 [==============================] - 64s 21ms/step - loss: 0.8262 - val_loss: 0.8744 - lr: 1.0000e-04\n",
      "Epoch 23/200\n",
      "3133/3133 [==============================] - ETA: 0s - loss: 0.8230 \n",
      "Epoch 23: val_loss did not improve from 0.85326\n",
      "3133/3133 [==============================] - 67s 21ms/step - loss: 0.8230 - val_loss: 0.8969 - lr: 1.0000e-04\n",
      "Epoch 24/200\n",
      "3132/3133 [============================>.] - ETA: 0s - loss: 0.8198  \n",
      "Epoch 24: val_loss did not improve from 0.85326\n",
      "3133/3133 [==============================] - 66s 21ms/step - loss: 0.8198 - val_loss: 0.8898 - lr: 1.0000e-04\n",
      "bi_lstm model saved!\n",
      "Training gru model...\n",
      "Epoch 1/200\n",
      "3132/3133 [============================>.] - ETA: 0s - loss: 1.2060     \n",
      "Epoch 1: val_loss improved from inf to 0.87188, saving model to datasets\\UK\\aurn_cam\\models\\gru.h5\n",
      "3133/3133 [==============================] - 26s 7ms/step - loss: 1.2060 - val_loss: 0.8719 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "3128/3133 [============================>.] - ETA: 0s - loss: 0.9517 \n",
      "Epoch 2: val_loss did not improve from 0.87188\n",
      "3133/3133 [==============================] - 22s 7ms/step - loss: 0.9516 - val_loss: 0.8928 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "3133/3133 [==============================] - ETA: 0s - loss: 0.9397 \n",
      "Epoch 3: val_loss did not improve from 0.87188\n",
      "3133/3133 [==============================] - 25s 8ms/step - loss: 0.9397 - val_loss: 0.9025 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "3133/3133 [==============================] - ETA: 0s - loss: 0.9317 \n",
      "Epoch 4: val_loss did not improve from 0.87188\n",
      "3133/3133 [==============================] - 23s 7ms/step - loss: 0.9317 - val_loss: 0.8731 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "3128/3133 [============================>.] - ETA: 0s - loss: 0.9252 \n",
      "Epoch 5: val_loss did not improve from 0.87188\n",
      "3133/3133 [==============================] - 23s 7ms/step - loss: 0.9252 - val_loss: 0.8941 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "3128/3133 [============================>.] - ETA: 0s - loss: 0.9206 \n",
      "Epoch 6: val_loss did not improve from 0.87188\n",
      "3133/3133 [==============================] - 24s 8ms/step - loss: 0.9207 - val_loss: 0.8759 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "3125/3133 [============================>.] - ETA: 0s - loss: 0.9165 \n",
      "Epoch 7: val_loss did not improve from 0.87188\n",
      "3133/3133 [==============================] - 23s 7ms/step - loss: 0.9164 - val_loss: 0.8983 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "3132/3133 [============================>.] - ETA: 0s - loss: 0.9136 \n",
      "Epoch 8: val_loss did not improve from 0.87188\n",
      "3133/3133 [==============================] - 25s 8ms/step - loss: 0.9137 - val_loss: 0.8750 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "3126/3133 [============================>.] - ETA: 0s - loss: 0.9099 \n",
      "Epoch 9: val_loss did not improve from 0.87188\n",
      "3133/3133 [==============================] - 22s 7ms/step - loss: 0.9098 - val_loss: 0.8913 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "3129/3133 [============================>.] - ETA: 0s - loss: 0.9097 \n",
      "Epoch 10: val_loss did not improve from 0.87188\n",
      "3133/3133 [==============================] - 22s 7ms/step - loss: 0.9098 - val_loss: 0.8838 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "3130/3133 [============================>.] - ETA: 0s - loss: 0.9040 \n",
      "Epoch 11: val_loss did not improve from 0.87188\n",
      "3133/3133 [==============================] - 23s 7ms/step - loss: 0.9039 - val_loss: 0.8803 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "3132/3133 [============================>.] - ETA: 0s - loss: 0.8825 \n",
      "Epoch 12: val_loss did not improve from 0.87188\n",
      "3133/3133 [==============================] - 24s 8ms/step - loss: 0.8826 - val_loss: 0.8794 - lr: 1.0000e-04\n",
      "Epoch 13/200\n",
      "3131/3133 [============================>.] - ETA: 0s - loss: 0.8796 \n",
      "Epoch 13: val_loss did not improve from 0.87188\n",
      "3133/3133 [==============================] - 23s 7ms/step - loss: 0.8796 - val_loss: 0.8813 - lr: 1.0000e-04\n",
      "Epoch 14/200\n",
      "3129/3133 [============================>.] - ETA: 0s - loss: 0.8780 \n",
      "Epoch 14: val_loss did not improve from 0.87188\n",
      "3133/3133 [==============================] - 23s 7ms/step - loss: 0.8780 - val_loss: 0.8874 - lr: 1.0000e-04\n",
      "Epoch 15/200\n",
      "3128/3133 [============================>.] - ETA: 0s - loss: 0.8768 \n",
      "Epoch 15: val_loss did not improve from 0.87188\n",
      "3133/3133 [==============================] - 22s 7ms/step - loss: 0.8767 - val_loss: 0.8911 - lr: 1.0000e-04\n",
      "Epoch 16/200\n",
      "3127/3133 [============================>.] - ETA: 0s - loss: 0.8750 \n",
      "Epoch 16: val_loss did not improve from 0.87188\n",
      "3133/3133 [==============================] - 22s 7ms/step - loss: 0.8752 - val_loss: 0.8895 - lr: 1.0000e-04\n",
      "gru model saved!\n",
      "Training bi_gru model...\n",
      "Epoch 1/200\n",
      "3132/3133 [============================>.] - ETA: 0s - loss: 1.2639     \n",
      "Epoch 1: val_loss improved from inf to 0.89831, saving model to datasets\\UK\\aurn_cam\\models\\bi_gru.h5\n",
      "3133/3133 [==============================] - 59s 16ms/step - loss: 1.2639 - val_loss: 0.8983 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "3129/3133 [============================>.] - ETA: 0s - loss: 0.9745  \n",
      "Epoch 2: val_loss improved from 0.89831 to 0.88935, saving model to datasets\\UK\\aurn_cam\\models\\bi_gru.h5\n",
      "3133/3133 [==============================] - 47s 15ms/step - loss: 0.9746 - val_loss: 0.8893 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "3132/3133 [============================>.] - ETA: 0s - loss: 0.9626 \n",
      "Epoch 3: val_loss did not improve from 0.88935\n",
      "3133/3133 [==============================] - 55s 17ms/step - loss: 0.9626 - val_loss: 0.9019 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "3131/3133 [============================>.] - ETA: 0s - loss: 0.9537 \n",
      "Epoch 4: val_loss improved from 0.88935 to 0.87306, saving model to datasets\\UK\\aurn_cam\\models\\bi_gru.h5\n",
      "3133/3133 [==============================] - 49s 16ms/step - loss: 0.9537 - val_loss: 0.8731 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "3130/3133 [============================>.] - ETA: 0s - loss: 0.9492 \n",
      "Epoch 5: val_loss improved from 0.87306 to 0.86762, saving model to datasets\\UK\\aurn_cam\\models\\bi_gru.h5\n",
      "3133/3133 [==============================] - 48s 15ms/step - loss: 0.9492 - val_loss: 0.8676 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "3130/3133 [============================>.] - ETA: 0s - loss: 0.9466  \n",
      "Epoch 6: val_loss did not improve from 0.86762\n",
      "3133/3133 [==============================] - 48s 15ms/step - loss: 0.9467 - val_loss: 0.8894 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "3132/3133 [============================>.] - ETA: 0s - loss: 0.9470  \n",
      "Epoch 7: val_loss did not improve from 0.86762\n",
      "3133/3133 [==============================] - 49s 16ms/step - loss: 0.9470 - val_loss: 0.8815 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "3131/3133 [============================>.] - ETA: 0s - loss: 0.9424 \n",
      "Epoch 8: val_loss did not improve from 0.86762\n",
      "3133/3133 [==============================] - 52s 17ms/step - loss: 0.9424 - val_loss: 0.8964 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "3133/3133 [==============================] - ETA: 0s - loss: 0.9369 \n",
      "Epoch 9: val_loss improved from 0.86762 to 0.85902, saving model to datasets\\UK\\aurn_cam\\models\\bi_gru.h5\n",
      "3133/3133 [==============================] - 67s 21ms/step - loss: 0.9369 - val_loss: 0.8590 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "3131/3133 [============================>.] - ETA: 0s - loss: 0.9360  \n",
      "Epoch 10: val_loss did not improve from 0.85902\n",
      "3133/3133 [==============================] - 70s 22ms/step - loss: 0.9360 - val_loss: 0.8635 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "3132/3133 [============================>.] - ETA: 0s - loss: 0.9316  \n",
      "Epoch 11: val_loss did not improve from 0.85902\n",
      "3133/3133 [==============================] - 72s 23ms/step - loss: 0.9316 - val_loss: 0.8996 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "3133/3133 [==============================] - ETA: 0s - loss: 0.9304  \n",
      "Epoch 12: val_loss did not improve from 0.85902\n",
      "3133/3133 [==============================] - 75s 24ms/step - loss: 0.9304 - val_loss: 0.8735 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "3131/3133 [============================>.] - ETA: 0s - loss: 0.9258 \n",
      "Epoch 13: val_loss did not improve from 0.85902\n",
      "3133/3133 [==============================] - 72s 23ms/step - loss: 0.9257 - val_loss: 0.8704 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "3132/3133 [============================>.] - ETA: 0s - loss: 0.9287  \n",
      "Epoch 14: val_loss did not improve from 0.85902\n",
      "3133/3133 [==============================] - 69s 22ms/step - loss: 0.9287 - val_loss: 0.9080 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "3133/3133 [==============================] - ETA: 0s - loss: 0.9259  \n",
      "Epoch 15: val_loss did not improve from 0.85902\n",
      "3133/3133 [==============================] - 58s 19ms/step - loss: 0.9259 - val_loss: 0.8811 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "3131/3133 [============================>.] - ETA: 0s - loss: 0.9249  \n",
      "Epoch 16: val_loss did not improve from 0.85902\n",
      "3133/3133 [==============================] - 60s 19ms/step - loss: 0.9250 - val_loss: 0.8950 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "3132/3133 [============================>.] - ETA: 0s - loss: 0.9242  \n",
      "Epoch 17: val_loss did not improve from 0.85902\n",
      "3133/3133 [==============================] - 76s 24ms/step - loss: 0.9242 - val_loss: 0.8860 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "3131/3133 [============================>.] - ETA: 0s - loss: 0.9192  \n",
      "Epoch 18: val_loss did not improve from 0.85902\n",
      "3133/3133 [==============================] - 53s 17ms/step - loss: 0.9193 - val_loss: 0.8794 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "3132/3133 [============================>.] - ETA: 0s - loss: 0.9239  \n",
      "Epoch 19: val_loss did not improve from 0.85902\n",
      "3133/3133 [==============================] - 68s 22ms/step - loss: 0.9238 - val_loss: 0.8707 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "3132/3133 [============================>.] - ETA: 0s - loss: 0.9002  \n",
      "Epoch 20: val_loss improved from 0.85902 to 0.85722, saving model to datasets\\UK\\aurn_cam\\models\\bi_gru.h5\n",
      "3133/3133 [==============================] - 90s 29ms/step - loss: 0.9002 - val_loss: 0.8572 - lr: 1.0000e-04\n",
      "Epoch 21/200\n",
      "3132/3133 [============================>.] - ETA: 0s - loss: 0.8935  \n",
      "Epoch 21: val_loss did not improve from 0.85722\n",
      "3133/3133 [==============================] - 72s 23ms/step - loss: 0.8934 - val_loss: 0.8644 - lr: 1.0000e-04\n",
      "Epoch 22/200\n",
      "3133/3133 [==============================] - ETA: 0s - loss: 0.8920  \n",
      "Epoch 22: val_loss did not improve from 0.85722\n",
      "3133/3133 [==============================] - 91s 29ms/step - loss: 0.8920 - val_loss: 0.8630 - lr: 1.0000e-04\n",
      "Epoch 23/200\n",
      "3132/3133 [============================>.] - ETA: 0s - loss: 0.8898  \n",
      "Epoch 23: val_loss did not improve from 0.85722\n",
      "3133/3133 [==============================] - 91s 29ms/step - loss: 0.8898 - val_loss: 0.8615 - lr: 1.0000e-04\n",
      "Epoch 24/200\n",
      "3132/3133 [============================>.] - ETA: 0s - loss: 0.8883  \n",
      "Epoch 24: val_loss did not improve from 0.85722\n",
      "3133/3133 [==============================] - 92s 29ms/step - loss: 0.8883 - val_loss: 0.8722 - lr: 1.0000e-04\n",
      "Epoch 25/200\n",
      "3133/3133 [==============================] - ETA: 0s - loss: 0.8868  \n",
      "Epoch 25: val_loss did not improve from 0.85722\n",
      "3133/3133 [==============================] - 91s 29ms/step - loss: 0.8868 - val_loss: 0.8709 - lr: 1.0000e-04\n",
      "Epoch 26/200\n",
      "3132/3133 [============================>.] - ETA: 0s - loss: 0.8858  \n",
      "Epoch 26: val_loss did not improve from 0.85722\n",
      "3133/3133 [==============================] - 67s 21ms/step - loss: 0.8858 - val_loss: 0.8763 - lr: 1.0000e-04\n",
      "Epoch 27/200\n",
      "3131/3133 [============================>.] - ETA: 0s - loss: 0.8854  \n",
      "Epoch 27: val_loss did not improve from 0.85722\n",
      "3133/3133 [==============================] - 62s 20ms/step - loss: 0.8854 - val_loss: 0.8709 - lr: 1.0000e-04\n",
      "Epoch 28/200\n",
      "3131/3133 [============================>.] - ETA: 0s - loss: 0.8839  \n",
      "Epoch 28: val_loss did not improve from 0.85722\n",
      "3133/3133 [==============================] - 65s 21ms/step - loss: 0.8839 - val_loss: 0.8650 - lr: 1.0000e-04\n",
      "Epoch 29/200\n",
      "3131/3133 [============================>.] - ETA: 0s - loss: 0.8831  \n",
      "Epoch 29: val_loss did not improve from 0.85722\n",
      "3133/3133 [==============================] - 64s 20ms/step - loss: 0.8831 - val_loss: 0.8709 - lr: 1.0000e-04\n",
      "Epoch 30/200\n",
      "3133/3133 [==============================] - ETA: 0s - loss: 0.8824  \n",
      "Epoch 30: val_loss did not improve from 0.85722\n",
      "3133/3133 [==============================] - 61s 19ms/step - loss: 0.8824 - val_loss: 0.8661 - lr: 1.0000e-04\n",
      "Epoch 31/200\n",
      "3131/3133 [============================>.] - ETA: 0s - loss: 0.8786 \n",
      "Epoch 31: val_loss did not improve from 0.85722\n",
      "3133/3133 [==============================] - 52s 17ms/step - loss: 0.8788 - val_loss: 0.8702 - lr: 1.0000e-05\n",
      "Epoch 32/200\n",
      "3133/3133 [==============================] - ETA: 0s - loss: 0.8776 \n",
      "Epoch 32: val_loss did not improve from 0.85722\n",
      "3133/3133 [==============================] - 52s 16ms/step - loss: 0.8776 - val_loss: 0.8701 - lr: 1.0000e-05\n",
      "Epoch 33/200\n",
      "3130/3133 [============================>.] - ETA: 0s - loss: 0.8771 \n",
      "Epoch 33: val_loss did not improve from 0.85722\n",
      "3133/3133 [==============================] - 52s 17ms/step - loss: 0.8771 - val_loss: 0.8752 - lr: 1.0000e-05\n",
      "Epoch 34/200\n",
      "3132/3133 [============================>.] - ETA: 0s - loss: 0.8766 \n",
      "Epoch 34: val_loss did not improve from 0.85722\n",
      "3133/3133 [==============================] - 51s 16ms/step - loss: 0.8767 - val_loss: 0.8728 - lr: 1.0000e-05\n",
      "Epoch 35/200\n",
      "3130/3133 [============================>.] - ETA: 0s - loss: 0.8763 \n",
      "Epoch 35: val_loss did not improve from 0.85722\n",
      "3133/3133 [==============================] - 54s 17ms/step - loss: 0.8762 - val_loss: 0.8704 - lr: 1.0000e-05\n",
      "bi_gru model saved!\n"
     ]
    }
   ],
   "source": [
    "def train_and_save_models(models_folder, sequence_length, X_train, y_train, X_test, y_test, epochs=500):\n",
    "    models = {\n",
    "        \"lstm\": lstm_model(sequence_length, X_train.shape[2]),\n",
    "        \"bi_lstm\": bi_lstm_model(sequence_length, X_train.shape[2]),\n",
    "        \"gru\": gru_model(sequence_length, X_train.shape[2]),\n",
    "        \"bi_gru\": bi_gru_model(sequence_length, X_train.shape[2]),\n",
    "    }\n",
    "    \n",
    "    os.makedirs(Path(models_folder), exist_ok=True)\n",
    "    huber = tf.keras.losses.Huber(delta=0.1)\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Training {model_name} model...\")\n",
    "        model.compile(optimizer=\"adam\", loss=huber)\n",
    "        \n",
    "        callbacks_list = [\n",
    "            callbacks.ModelCheckpoint(\n",
    "                filepath=Path(models_folder) / f\"{model_name}.h5\",\n",
    "                monitor=\"val_loss\",\n",
    "                verbose=1,\n",
    "                save_weights_only=False,\n",
    "                save_best_only=True,\n",
    "            ),\n",
    "            callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.1,\n",
    "                patience=10,\n",
    "                min_lr=0.0\n",
    "            ),\n",
    "            callbacks.EarlyStopping(\n",
    "                monitor='val_loss',        \n",
    "                patience=15,                \n",
    "                min_delta=0.001,           \n",
    "                restore_best_weights=True   \n",
    "            )            \n",
    "        ]\n",
    "        \n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_test, y_test),\n",
    "            epochs=epochs,\n",
    "            batch_size=32, ## TODO: Change\n",
    "            callbacks=callbacks_list\n",
    "        )\n",
    "        \n",
    "        model.save(Path(models_folder) / f\"{model_name}.h5\")\n",
    "        print(f\"{model_name} model saved!\")\n",
    "\n",
    "# train the model\n",
    "train_and_save_models(\n",
    "    models_folder, \n",
    "    sequence_length, \n",
    "    X_train, y_train, \n",
    "    X_test, y_test,\n",
    "    epochs=200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0656be49-2160-445e-b306-0e255a9ec9b3",
   "metadata": {},
   "source": [
    "### 5. Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "055eaec9-3fe6-4164-91a3-a1df669e9a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating lstm model...\n",
      "386/386 [==============================] - 2s 4ms/step\n",
      "Evaluating bi_lstm model...\n",
      "386/386 [==============================] - 6s 10ms/step\n",
      "Evaluating gru model...\n",
      "386/386 [==============================] - 1s 3ms/step\n",
      "Evaluating bi_gru model...\n",
      "386/386 [==============================] - 4s 5ms/step\n",
      "Evaluation results saved in directory: datasets\\UK\\aurn_cam\\results\\bi_gru\n"
     ]
    }
   ],
   "source": [
    "def evaluate_all_models(models_folder, X_test, y_test, X_test_meta, output_dir, site_column=\"SiteNumber\"):\n",
    "       \n",
    "    models = {model_name: tf.keras.models.load_model(Path(models_folder) / f\"{model_name}.h5\") \n",
    "              for model_name in [\"lstm\", \"bi_lstm\", \"gru\", \"bi_gru\"]}\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        output_folder = output_dir/Path(model_name)\n",
    "        Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(f\"Evaluating {model_name} model...\")\n",
    "        results = []\n",
    "        predictions_list = []  # Store predictions for CSV\n",
    "    \n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        metrics = {\n",
    "            \"Model\": model_name,\n",
    "            \"RMSE\": np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "            \"MAE\": mean_absolute_error(y_test, y_pred),\n",
    "            \"MAPE\": np.mean(np.abs((y_test - y_pred) / y_test)) * 100,\n",
    "            \"R-squared\": r2_score(y_test, y_pred)\n",
    "        }\n",
    "        results.append(metrics)\n",
    "        pd.DataFrame(results).to_csv(Path(output_folder) / f\"{model_name}_metrics.csv\", index=False)\n",
    "        \n",
    "\n",
    "        if isinstance(X_test_meta, np.ndarray):\n",
    "            expected_columns=[\"Start_Time\", \"End_Time\", \"SiteNumber\"]\n",
    "            X_test_meta = pd.DataFrame(X_test_meta, columns=expected_columns)\n",
    "        \n",
    "        # Collect data for CSV\n",
    "        predictions_df = pd.DataFrame({\n",
    "            \"Time\": X_test_meta[\"End_Time\"].values.flatten(),\n",
    "            \"SiteNumber\": X_test_meta[\"SiteNumber\"].values.flatten(),\n",
    "            \"True_Value\": y_test.flatten(),\n",
    "            \"Predicted_Value\": y_pred.flatten()\n",
    "        })\n",
    "        predictions_list.append(predictions_df)\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(X_test_meta[\"End_Time\"], y_test, label=\"True Values\", color=\"blue\")\n",
    "        plt.plot(X_test_meta[\"End_Time\"], y_pred.flatten(), label=\"Predictions\", color=\"red\")\n",
    "        plt.title(f\"{model_name} Predictions vs True Values\")\n",
    "        plt.xlabel(\"Dates\")\n",
    "        plt.ylabel(\"Ground Ozone\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.savefig(Path(output_folder) / f\"{model_name}_predictions.pdf\", format=\"pdf\")\n",
    "        plt.close()\n",
    "        \n",
    "        \n",
    "        # Save all predictions for this model\n",
    "        all_predictions_df = pd.concat(predictions_list, ignore_index=True)\n",
    "        \n",
    "        # Apply function to each row to get the SiteName, Longitude and Latitudes\n",
    "        all_predictions_df[['SiteName', 'Longitude', 'Latitude']] = all_predictions_df['SiteNumber'].apply(\n",
    "            lambda x: pd.Series(get_site_info(x, stations_file))\n",
    "        )\n",
    "\n",
    "        all_predictions_df.to_csv(output_folder / f\"{model_name}_predictions.csv\", index=False)\n",
    "\n",
    "    print(f\"Evaluation results saved in directory: {output_folder}\")\n",
    "\n",
    "\n",
    "output_dir=Path(dataset_folder)/Path(model_dataset)/Path('results')\n",
    "evaluate_all_models(\n",
    "    models_folder, \n",
    "    X_test, y_test, X_test_meta, \n",
    "    output_dir=output_dir,\n",
    "    site_column=\"SiteNumber\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c1bbe7-d1ef-4c73-9906-5117f56d9d89",
   "metadata": {},
   "source": [
    "### 6. Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f6154832-1c1b-42f7-b30a-4cfd2555739b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Average\n",
    "from keras.models import Model\n",
    "\n",
    "def ensemble_model(models):\n",
    "    # Get the inputs and outputs of the individual models\n",
    "    inputs = [model.input for model in models]\n",
    "    predictions = [model.output for model in models]\n",
    "    \n",
    "    # Average the predictions\n",
    "    ensemble_output = Average()(predictions)\n",
    "    \n",
    "    # Create the ensemble model\n",
    "    ensemble = Model(inputs=inputs, outputs=ensemble_output)\n",
    "    \n",
    "    return ensemble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad99732-55e3-4b8e-8fb1-24d4edaecf08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "3133/3133 [==============================] - ETA: 0s - loss: 1.2828      \n",
      "Epoch 1: val_loss improved from inf to 0.91004, saving model to datasets\\UK\\aurn_cam\\models\\ensemble.h5\n",
      "3133/3133 [==============================] - 179s 50ms/step - loss: 1.2828 - val_loss: 0.9100 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "2862/3133 [==========================>...] - ETA: 12s - loss: 0.9543 "
     ]
    }
   ],
   "source": [
    "def train_ensemble_model(X_train, y_train, X_test, y_test, sequence_length, models_folder, model_fns, epochs=500):\n",
    "    # Create each model dynamically using the passed functions\n",
    "    models = [model_fn(sequence_length, X_train.shape[2]) for model_fn in model_fns]\n",
    "\n",
    "    # Create the ensemble model\n",
    "    model = ensemble_model(models)  # Assuming ensemble_model function takes a list of models\n",
    "\n",
    "    # Define the loss function and compile the model\n",
    "    huber = tf.keras.losses.Huber(delta=0.1)\n",
    "    model.compile(optimizer='adam', loss=huber)\n",
    "\n",
    "    # Create the model directory if it doesn't exist\n",
    "    os.makedirs(Path(models_folder), exist_ok=True)\n",
    "    path_checkpoint = Path(models_folder) / Path(\"ensemble.h5\")\n",
    "\n",
    "    # Define the callbacks\n",
    "    reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', \n",
    "        factor=0.1, \n",
    "        patience=10, \n",
    "        min_lr=0.0\n",
    "    )\n",
    "\n",
    "    modelckpt_callback = callbacks.ModelCheckpoint(\n",
    "        monitor=\"val_loss\",\n",
    "        filepath=path_checkpoint,\n",
    "        verbose=1,\n",
    "        save_weights_only=False,\n",
    "        save_best_only=True,\n",
    "    )\n",
    "\n",
    "    early_stopping = callbacks.EarlyStopping(\n",
    "        monitor='val_loss',        \n",
    "        patience=15,                \n",
    "        min_delta=0.001,           \n",
    "        restore_best_weights=True   \n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        [X_train, X_train, X_train, X_train], y_train, \n",
    "        validation_data=([X_test, X_test, X_test, X_test], y_test), \n",
    "        epochs=epochs, \n",
    "        batch_size=32, \n",
    "        callbacks=[modelckpt_callback, reduce_lr, early_stopping]\n",
    "    )\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "\n",
    "model_fns = [lstm_model, bi_lstm_model, gru_model, bi_gru_model]  # Pass model creation functions as a list\n",
    "history = train_ensemble_model(\n",
    "    X_train, y_train, \n",
    "    X_test, y_test, \n",
    "    sequence_length, \n",
    "    models_folder, \n",
    "    model_fns, \n",
    "    epochs=500\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03406d7e-f102-42ac-996d-3f853056dfdb",
   "metadata": {},
   "source": [
    "### 7. Evaluate Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c4334a0-79ed-4451-93eb-8cd5eb1f2c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets\\UK\\aurn_cam\\models\\ensemble.h5\n",
      "Evaluating ensemble model...\n",
      "386/386 [==============================] - 12s 20ms/step\n",
      "Evaluation results saved in directory: datasets\\UK\\aurn_cam\\results\\ensemble\n"
     ]
    }
   ],
   "source": [
    "def evaluate_ensemble(models_folder, X_test, y_test, X_test_meta, output_dir, model_name, site_column=\"SiteNumber\"):\n",
    "    print(Path(models_folder) / f\"{model_name}.h5\")\n",
    "    model = tf.keras.models.load_model(Path(models_folder) / f\"{model_name}.h5\") \n",
    "    \n",
    "    output_folder = output_dir/Path(model_name)\n",
    "    Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    print(f\"Evaluating {model_name} model...\")\n",
    "    results = []\n",
    "    predictions_list = []\n",
    "        \n",
    "    y_pred = model.predict([X_test, X_test, X_test, X_test])\n",
    "    \n",
    "    metrics = {\n",
    "        \"Model\": model_name,\n",
    "        \"RMSE\": np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "        \"MAE\": mean_absolute_error(y_test, y_pred),\n",
    "        \"MAPE\": np.mean(np.abs((y_test - y_pred) / y_test)) * 100,\n",
    "        \"R-squared\": r2_score(y_test, y_pred)\n",
    "    }\n",
    "    results.append(metrics)\n",
    "    pd.DataFrame(results).to_csv(Path(output_folder) / f\"{model_name}_metrics.csv\", index=False)\n",
    "\n",
    "    if isinstance(X_test_meta, np.ndarray):\n",
    "            expected_columns=[\"Start_Time\", \"End_Time\", \"SiteNumber\"]\n",
    "            X_test_meta = pd.DataFrame(X_test_meta, columns=expected_columns)\n",
    "    \n",
    "    # Collect data for CSV\n",
    "    predictions_df = pd.DataFrame({\n",
    "        \"Time\": X_test_meta[\"End_Time\"].values.flatten(),\n",
    "        \"SiteNumber\": X_test_meta[\"SiteNumber\"].values.flatten(),\n",
    "        \"True_Value\": y_test.flatten(),\n",
    "        \"Predicted_Value\": y_pred.flatten()\n",
    "    })\n",
    "    predictions_list.append(predictions_df)\n",
    "        \n",
    "    # Save all predictions for this model\n",
    "    all_predictions_df = pd.concat(predictions_list, ignore_index=True)\n",
    "    \n",
    "    # Apply function to each row to get the SiteName, Longitude and Latitudes\n",
    "    all_predictions_df[['SiteName', 'Longitude', 'Latitude']] = all_predictions_df['SiteNumber'].apply(\n",
    "        lambda x: pd.Series(get_site_info(x, stations_file))\n",
    "    )\n",
    "\n",
    "    all_predictions_df.to_csv(output_folder / f\"{model_name}_predictions.csv\", index=False)\n",
    "\n",
    "    print(f\"Evaluation results saved in directory: {output_folder}\")\n",
    "\n",
    "\n",
    "output_dir=Path(dataset_folder)/Path(model_dataset)/Path('results')\n",
    "evaluate_ensemble(\n",
    "    models_folder, \n",
    "    X_test, y_test, X_test_meta,\n",
    "    output_dir, \n",
    "    model_name=\"ensemble\",\n",
    "    site_column=\"SiteNumber\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3ed62f-c3d6-4a26-960d-cf778803ba70",
   "metadata": {},
   "source": [
    "## 7. Final: Post Processing of Results\n",
    "The models we have develop predict as single value for Ozone. Can the model performance be improved by binning the prediction into deciles similar to the one used by AURN for air pollution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77aaa9d5-7f37-49e6-9a14-eded14ddd622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bin_numbers(go3, go3_min=1, go3_max=238, bins=4):\n",
    "    # Normalize the value of go3 in the range [0, 1]\n",
    "    normalized_value = (go3 - go3_min) / (go3_max - go3_min)\n",
    "    \n",
    "    # Multiply by the number of bins to get a value between 0 and bins-1\n",
    "    decile = normalized_value * bins\n",
    "    \n",
    "    # Return the decile number (rounded to the nearest integer)\n",
    "    return min(bins - 1, round(decile))  # Ensure the decile is within the range [0, bins-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c9a080e3-5a4b-485f-93e1-70867ced3339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deciles added for lstm and saved to datasets\\UK\\aurn_cam\\results\\lstm\\lstm_predictions.csv\n",
      "Deciles added for bi_lstm and saved to datasets\\UK\\aurn_cam\\results\\bi_lstm\\bi_lstm_predictions.csv\n",
      "Deciles added for gru and saved to datasets\\UK\\aurn_cam\\results\\gru\\gru_predictions.csv\n",
      "Deciles added for bi_gru and saved to datasets\\UK\\aurn_cam\\results\\bi_gru\\bi_gru_predictions.csv\n",
      "Deciles added for ensemble and saved to datasets\\UK\\aurn_cam\\results\\ensemble\\ensemble_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "for model_name in [\"lstm\", \"bi_lstm\", \"gru\", \"bi_gru\", \"ensemble\"]:\n",
    "    # Define the path for the output directory and the predictions CSV file\n",
    "    output_dir = Path(dataset_folder)/Path(model_dataset)/Path('results') / model_name / f\"{model_name}_predictions.csv\"\n",
    "    decile_metrics_file = Path(dataset_folder)/Path(model_dataset)/Path('results') / model_name / f\"{model_name}__metrics_deciles.csv\"\n",
    "    \n",
    "    # Open the CSV as a DataFrame\n",
    "    df = pd.read_csv(output_dir)\n",
    "    \n",
    "    # Apply the deciles calculation using the 'go3' column\n",
    "    df['Predicted_Value_Decile'] = df['Predicted_Value'].apply(get_bin_numbers)\n",
    "    df['True_Value_Decile'] = df['True_Value'].apply(get_bin_numbers)\n",
    "\n",
    "    # Save the DataFrame back to the same file\n",
    "    df.to_csv(output_dir, index=False)\n",
    "    \n",
    "    # print or log a message indicating that the deciles were applied and the file was saved\n",
    "    print(f\"Deciles added for {model_name} and saved to {output_dir}\")\n",
    "\n",
    "    # Ensure necessary columns exist\n",
    "    if 'True_Value' in df.columns and 'Predicted_Value' in df.columns:\n",
    "        y_true = df['True_Value_Decile']\n",
    "        y_pred = df['Predicted_Value_Decile']\n",
    "        \n",
    "       # Compute classification metrics\n",
    "        metrics = {\n",
    "            \"Model\": model_name,\n",
    "            \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "            \"Precision\": precision_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "            \"Recall\": recall_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "            \"F1-Score\": f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "        }\n",
    "\n",
    "        # Save metrics to a CSV file\n",
    "        metrics_df = pd.DataFrame([metrics])\n",
    "        metrics_df.to_csv(decile_metrics_file, header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184c2b24-3a84-4921-bc1a-f9f2b13302fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19618dd9-df6b-4ecc-b458-cd8c7271b7c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
